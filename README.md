#  Synthetic Data Analytics Project


##  Project Objective

- Simulate real-world data analysis using synthetic datasets
- Apply theoretical knowledge in Python, ETL, and visualization tools
- Practice end-to-end integration of data platforms (Airflow â†’ ClickHouse â†’ Superset)
- Build informative dashboards based on simulated social datasets

---

##  Tools & Technologies

| Category        | Tools Used                            |
|----------------|----------------------------------------|
| Programming     | Python 3.8+                           |
| Data Processing | Pandas, NumPy                         |
| Synthetic Data  | SDV, CTGAN, GaussianCopulaSynthesizer |
| ETL             | Apache Airflow                        |
| Database        | ClickHouse                            |
| BI Dashboard    | Apache Superset                       |
| Containerization| Docker, Docker Compose                |

---

## Repository Structure
synthetic-project/
â”œâ”€â”€ airflow/dags/ # Airflow DAG for data load
â”‚ â””â”€â”€ cks_load_to_clickhouse.py
â”œâ”€â”€ assets/ # Superset dashboard screenshots
â”‚ â””â”€â”€ *.jpg
â”œâ”€â”€ clickhouse_engine/ # ClickHouse connection engine
â”‚ â””â”€â”€ clickhouse_engine.py
â”œâ”€â”€ data/ # Final datasets
â”‚ â””â”€â”€ cks_synthetic.csv
â”‚ â””â”€â”€ kezekte_dummy.csv
â”‚ â””â”€â”€ e_obr_dummy.csv
â”‚ â””â”€â”€ CKS.csv # Original reference
â”œâ”€â”€ generator/ # Data synthesis code
â”‚ â””â”€â”€ *.py
â”œâ”€â”€ docker-compose.yml # Full stack configuration
â”œâ”€â”€ Dockerfile, Dockerfile.airflow
â”œâ”€â”€ requirements.airflow.txt, requirements.superset.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## ğŸ“¦ Dataset Descriptions

| File               | Description                                      |
|--------------------|--------------------------------------------------|
| `cks_synthetic.csv`| Family categories and filters dataset            |
| `kezekte_dummy.csv`| Social queue data of families in need            |
| `e_obr_dummy.csv`  | Government service application history           |

---

## âš™ï¸ How to Run Locally

1. **Clone the repo**:
   ```bash
   git clone https://github.com/your-username/synthetic-project.git
   cd synthetic-project



Start the services:

docker-compose up --build
Upload the CSV files (if not already in /data/).

Access the services:

Airflow: http://localhost:8080

Superset: http://localhost:8088

ClickHouse UI: http://localhost:8123

Run the Airflow DAG:
cks_load_to_clickhouse will load all .csv files into ClickHouse.

Log into Superset, connect to ClickHouse DB, and build or explore dashboards.